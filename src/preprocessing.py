import copy
import functools
from typing import Type

from pyspark.ml.feature import StringIndexer
from pyspark.sql import DataFrame, SparkSession
from pyspark.sql import functions as F
from pyspark.sql import types
from tsfresh.convenience.bindings import spark_feature_extraction_on_chunk
from tsfresh.feature_extraction.settings import MinimalFCParameters
from tsfresh.utilities.dataframe_functions import roll_time_series

INT_TYPES = (types.IntegerType, types.LongType)


class ArrayIndexer:
    """Encodes unique arrays into integer indices.

    This transformer takes an array column and produces a numeric
    index for each unique array (treated as a whole, not per-element).
    Internally, it joins the array into a single string using a delimiter
    before fitting a StringIndexer model.

    Parameters
    ----------
    input_col : str
        Name of the input array<string> column to encode.

    output_col : str
        Name of the output column that will store the numeric index.

    delimiter : str, optional, default="__"
        Delimiter to join array elements into a single string. Choose a value
        that does not appear in your data to avoid collisions.
    """

    def __init__(self, input_col, output_col, delimiter="__"):
        self.input_col = input_col
        self.output_col = output_col
        self.delimiter = delimiter
        self._model = None
        self._joined_col = f"{input_col}__joined"

    def fit(self, df: DataFrame):
        df_tmp = df.withColumn(
            self._joined_col,
            F.array_join(F.col(self.input_col), self.delimiter),
        )
        self._model = StringIndexer(
            inputCol=self._joined_col, outputCol=self.output_col
        ).fit(df_tmp)
        
        return self

    def transform(self, df: DataFrame) -> DataFrame:
        if self._model is None:
            raise ValueError("Must call fit() before transform().")
        
        df_tmp = df.withColumn(
            self._joined_col,
            F.array_join(F.col(self.input_col), self.delimiter),
        )
        return (
            self._model.transform(df_tmp)
            .drop(self._joined_col)  # remove temp column
            .withColumn(
                self.output_col, F.col(self.output_col).cast(types.LongType())
            )
        )


def check_data_type(
    df: DataFrame, column: str, expected: Type[types.DataType] | tuple
) -> None:
    """Checks data type matches the expected one."""
    actual = df.schema[column].dataType
    if not isinstance(actual, expected):
        raise ValueError(
            f"Wrong DataType for column '{column}': expected {expected}, "
            f"but got {type(actual)}"
        )


def transform(
    spark: SparkSession,
    input: str | DataFrame,
    column_id: str,
    column_sort: str,
    column_kind: str,
    column_value: str,
    max_timeshift: int = 10,
    min_timeshift: int = 10,
):

    if not isinstance(input, DataFrame):
        input = spark.read.csv(input, header=True)

    timeseries_container = spark_roll_time_series_on_chunk(
        input,
        column_id=column_id,
        column_sort=column_sort,
        max_timeshift=max_timeshift,
        min_timeshift=min_timeshift,
    )

    # Drop `column_id` in favor of the new ArrayType(LongType()) `id` column
    # generated by tsfresh.
    timeseries_container = timeseries_container.drop(column_id)

    # Encode `id` column.
    timeseries_container = (
        ArrayIndexer(input_col="id", output_col="column_id")
        .fit(timeseries_container)
        .transform(timeseries_container)
    )

    # Extract features.
    df_grouped = timeseries_container.groupby(["column_id", column_kind])
    features = spark_feature_extraction_on_chunk(
        df_grouped,
        column_id="column_id",
        column_kind=column_kind,
        column_sort=column_sort,
        column_value=column_value,
        default_fc_parameters=MinimalFCParameters(),
    )

    return features


def spark_roll_time_series_on_chunk(
    df: DataFrame,
    column_id: str,
    column_sort: str,
    rolling_direction: int = 1,
    max_timeshift: int | None = None,
    min_timeshift: int = 0,
    pooling_col: str | None = None,
) -> DataFrame:
    """Applies a UDF that rolls a time series DataFrame in Spark using tsfresh.

    Parameters
    ----------
    df : DataFrame
        Input spark dataframe.

    column_id : str
        The name of the column identifying different time series.

    column_sort : str
        The name of the column representing time order within each group.

    rolling_direction : int, optional
        The direction of rolling.

    max_timeshift : int or None, optional
        The maximum shift size to apply during rolling.
        If None, no upper limit is applied. Defaults to None.

    min_timeshift : int, optional
        The minimum shift size to apply. Defaults to 0 (no shift).

    pooling_col : str or None, optional
        Column used to group data for pandas UDF batching.
        Defaults to `column_id`.

    Returns
    -------
    DataFrame
        A Spark DataFrame with rolled time series data
    """
    tsfresh_id_field = types.StructField(
        "id", types.ArrayType(types.LongType())
    )
    return_schema = copy.deepcopy(df.schema).add(tsfresh_id_field)

    check_data_type(df, column_id, INT_TYPES)
    check_data_type(df, column_sort, INT_TYPES)

    udf = functools.partial(
        roll_time_series,
        column_id=column_id,
        column_sort=column_sort,
        rolling_direction=rolling_direction,
        max_timeshift=max_timeshift,
        min_timeshift=min_timeshift,
    )

    return df.groupBy(pooling_col or column_id).applyInPandas(
        udf, return_schema
    )




input_schema = types.StructType(
    [
        types.StructField("column_sort", types.LongType()),
        types.StructField("column_value", types.DoubleType()),
        types.StructField("column_id", types.LongType()),
    ]
)
spark = SparkSession.builder.getOrCreate()
input: DataFrame = spark.read.csv(
    "Dataset/input", header=False, schema=input_schema
)
input = input.withColumn("column_kind", F.lit("target"))

features = transform(
    spark,
    input,
    column_id="column_id",
    column_sort="column_sort",
    column_kind="column_kind",
    column_value="column_value",
)

features = features.groupby("column_id").pivot("variable").sum("value")

# Next steps:
# -----------
# 1. Inverse transform StringIndexer's until original column id is recovered.
# 2. Apply a regressor to each group in parallel. See https://community.databricks.com/t5/machine-learning/running-multiple-linear-regressions-in-parallel-speeding-up-for/td-p/6433
